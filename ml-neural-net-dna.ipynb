{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluating a fully connected neural network for DNA sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, average_precision_score\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "\n",
    "# Specify what we will be working on 'cpu' or 'cuda' (GPU)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "my_dataset = pd.read_csv(\"dna_sequence_dataset.csv\")\n",
    "\n",
    "# Specificy data (X) and targets (y), since this is an example, we only use the first 50000 samples, decrease\n",
    "# this number if it seems the the training goes slowly\n",
    "X = my_dataset[\"dna_sequence\"][:50000]\n",
    "y = my_dataset[\"target\"][:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data consists of DNA sequences, and the targets are binary (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    AAAAAAAAAATA\n",
       "1    AAAAAAAAAGAG\n",
       "2    AAAAAAAAATCA\n",
       "3    AAAAAAAAATTC\n",
       "4    AAAAAAAACAAA\n",
       "Name: dna_sequence, dtype: object"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our neural net, we need to split our dataset into a **training**, **validation** and **testing** set. \n",
    "\n",
    " - The training set is used to train our model (70% of dataset)\n",
    " - The validation set is used to determine the number of epochs that the model is trained for (10% of dataset)\n",
    " - The testing set is used to assess the model's performance on unseen data. (20% of dataset)\n",
    "\n",
    "The **Dataset_fcANN** class is a custom class that defines the data and target values (We will class this a sample from now on). \n",
    "This class has to be a child class from PyTorch's basic Dataset class and it should overwrite at least \n",
    "the **init**, **len** and **getitem** methods.\n",
    "\n",
    "In this case, our custom dataset class simply convert the DNA sequence into a binary matrix using the function **one_hot_sequence**, this matrix is then flattened and converted into a tensor (a multi-dimensional vector), and finallly it is moved to the device (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sequence(seq):\n",
    "    \"\"\"Convert a DNA sequence into a binary matrix.\n",
    "       \n",
    "    Args:\n",
    "        seq (str): input DNA sequence\n",
    "\n",
    "    Returns:\n",
    "        m (numpy array): binary matrix\n",
    "\n",
    "    \"\"\"\n",
    "    d = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    length = len(seq)\n",
    "    m = np.zeros((4, length), dtype='float32')\n",
    "    for i, b in enumerate(seq):\n",
    "        m[d[b]][i] = 1   \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_fcANN(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"Initialization\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"Generates one data and target pair\"\n",
    "        # Select DNA sequence at the position in the dataset specified by idx\n",
    "        data = self.X.iloc[idx]\n",
    "        \n",
    "        # Convert the DNA sequence into a binary matrix\n",
    "        data = one_hot_sequence(data)\n",
    "        \n",
    "        # Flatten the binary matrix and convert the numpy array into a PyTorch tensor\n",
    "        data = torch.from_numpy(data.flatten())\n",
    "            \n",
    "        # Move the tensor to the device, depending on the loss function that you are using,\n",
    "        # you may want to change the datatype \n",
    "        data = data.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Select the target value at the position in the dataset specified by idx, and convert into a numpy array\n",
    "        target = np.array(self.y.iloc[idx])\n",
    "        \n",
    "        # Convert the numpy array into a PyTorch tensor\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        # Move the tensor to the device\n",
    "        target = target.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Return data and target tensors as a tuple\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DataLoader** class defines how the samples are read by the neural network. We set the mini-batch size to 100, and we also set drop_last to True. drop_last ensures that if the last mini-batch of a dataset contains just 1 samples (i.e. number of samples % number of mini-batches = 1), then this data value is dropped so that no errors occur. I could calculate if it is necessary to do this for our example dataset, but since we have so many samples (), we can afford to drop one.\n",
    "\n",
    "We enable shuffle to ensure that there is no bias in the data before we make a split, and stratify is used to make sure that the distribution of our target values (0 and 1) is the same for each split (training, validation and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, shuffle=True, stratify=y)\n",
    "\n",
    "# Create training and validation set (7/8th of 80% = 70%, and 1/8th of 80% = 10%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size=7/8, test_size=1/8, shuffle=True, stratify=y_train_val)\n",
    "\n",
    "# Create dictionary with datasets for training, validation and testing set\n",
    "datasets = {\n",
    "    \"train\": Dataset_fcANN(X_train, y_train),\n",
    "    \"valid\": Dataset_fcANN(X_val, y_val),\n",
    "    \"test\": Dataset_fcANN(X_test, y_test)\n",
    "}\n",
    "\n",
    "# Set mini-batch size parameter\n",
    "batch_size = 100\n",
    "\n",
    "# Create dictionary with dataloaders for training, validation and testing set\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(datasets[\"train\"], batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    \"valid\": DataLoader(datasets[\"valid\"], batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    \"test\": DataLoader(datasets[\"test\"], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create fully-connect Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the most important part of this notebook: the fully-connected artificial neural network. This neural network includes 1 input layer, 1 hidden layer, 1 output layer, and 2 batch normalization layers.\n",
    "The output layer consists of 2 output nodes: one for each possible target value (0 or 1).\n",
    "\n",
    "This neural network is encoded by the **fcANN** class, which is a child class from PyTorch's nn.Module class, and it has to define a **forward** method. This method describes one forward pass of a data value through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcANN(nn.Module):\n",
    "    def __init__(self, in_size=48, hidden_size=192, out_size=2, dropoutp=0.2, epochs=0):\n",
    "        super(fcANN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(in_size, hidden_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropoutp)\n",
    "        self.fc2 = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "        # Define number of epochs that the model has been trained for\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.bn(self.fc1(x))))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fcANN(\n",
       "  (fc1): Linear(in_features=48, out_features=192, bias=True)\n",
       "  (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=192, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model using default parameters\n",
    "model = fcANN() \n",
    "\n",
    "# View model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the neural network, we use the custom **train** function. This function trains the neural network using the data in the dataloader dictionary that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          save_file_name,\n",
    "          max_epochs_stop=3,\n",
    "          n_epochs=3,\n",
    "          print_every=1):\n",
    "    \"\"\"Train a PyTorch neural network.\n",
    "       \n",
    "       This function is inspired by code found in this notebook:\n",
    "       https://github.com/WillKoehrsen/pytorch_challenge/blob/master/Transfer%20Learning%20in%20PyTorch.ipynb\n",
    "\n",
    "    Args:\n",
    "        model (PyTorch model): neural network to train\n",
    "        criterion (PyTorch loss): objective to minimize\n",
    "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
    "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "        n_epochs (int): maximum number of training epochs\n",
    "        print_every (int): frequency of epochs to print training stats\n",
    "\n",
    "    Returns:\n",
    "        model (PyTorch model): trained neural network   \n",
    "        \n",
    "    \"\"\"\n",
    "    ########################\n",
    "    # Initialize variables #\n",
    "    ######################## \n",
    "    \n",
    "    # Create timer\n",
    "    overall_start = timer()\n",
    "    \n",
    "    # Set number of epochs mdoel was trained for to 0\n",
    "    model.epochs = 0\n",
    "    \n",
    "    # Initialize variable for tracking how many times that \n",
    "    # the loss has not lowered after one epoch of training\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Initialize variable for tracking the validation loss\n",
    "    # We set this variable to +Infinity so that the first time that the model is trained, \n",
    "    # it will always be lower, and thus continue training\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    # Initialize variable for tracking if the training was stopped early\n",
    "    early_stopping = False\n",
    "    \n",
    "    # Initialize dictionary for tracking different metrics\n",
    "    # This dictionary should contain at least a \"loss\", \"target\", and \"output\" key.\n",
    "    metrics_dict = {'loss': {'train': [], 'valid': []},\n",
    "                    'target': {'train': [], 'valid': []}, \n",
    "                    'output': {'train': [], 'valid': []},\n",
    "                    'accuracy': {'train': [], 'valid': []},\n",
    "                    'roc_auc': {'train': [], 'valid': []},\n",
    "                    'pr_auc': {'train': [], 'valid': []}}\n",
    "    \n",
    "    assert all([x in metrics_dict for x in [\"loss\", \"target\", \"output\"]]), 'Metrics dictionary must contain at least a \"loss\", \"target\", and \"output\" key.'\n",
    "       \n",
    "    for epoch in range(n_epochs):\n",
    "                   \n",
    "        # Initialize metrics to record a new epoch\n",
    "        # We do this for both the training set and the validation set\n",
    "        for mode in ['train', 'valid']:\n",
    "            metrics_dict['loss'][mode].append(0)\n",
    "            # Value is 2, because we have 2 nodes in the output layer (output is numpy array with length 2)\n",
    "            metrics_dict['output'][mode].append(np.empty((0, 2))) \n",
    "            metrics_dict['target'][mode].append([])\n",
    "            metrics_dict['accuracy'][mode].append([])\n",
    "            metrics_dict['roc_auc'][mode].append([])\n",
    "            metrics_dict['pr_auc'][mode].append([])\n",
    "\n",
    "        # Set model to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "        \n",
    "        #################\n",
    "        # Training loop #\n",
    "        #################\n",
    "\n",
    "        for mini_batch_number, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Predicted outputs\n",
    "            output = model(data)\n",
    "            \n",
    "            # Loss and backpropagation of gradients\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "         \n",
    "            # Multiply average loss by number of examples in a minibatch\n",
    "            metrics_dict['loss']['train'][-1] += loss.item() * len(target)\n",
    "\n",
    "            # Append target and output to lists\n",
    "            metrics_dict['target']['train'][-1] = np.append(metrics_dict['target']['train'][-1], target.data.cpu().numpy())\n",
    "            metrics_dict['output']['train'][-1] = np.vstack((metrics_dict['output']['train'][-1], output.data.cpu().numpy()))\n",
    "        \n",
    "            # Print training completion percentage\n",
    "            print(f'Epoch: {epoch+1}\\t{100 * (mini_batch_number + 1) / len(train_loader):6.2f}% complete ({str(datetime.timedelta(seconds=round(timer() - start)))})', end='\\r')\n",
    "                \n",
    "        ###################\n",
    "        # Validation loop #\n",
    "        ###################\n",
    "        \n",
    "        # Don't need to keep track of gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            for data, target in valid_loader:\n",
    "                                \n",
    "                # Predicted outputs\n",
    "                output = model(data)\n",
    "\n",
    "                # Validation loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Multiply average loss by number of examples in a minibatch and add to metrics dictionary\n",
    "                metrics_dict['loss']['valid'][-1] += loss.item() * len(target)\n",
    "\n",
    "                # Append target and output to lists and add to metrics dictionary\n",
    "                metrics_dict['target']['valid'][-1] = np.append(metrics_dict['target']['valid'][-1], target)\n",
    "                metrics_dict['output']['valid'][-1] = np.vstack((metrics_dict['output']['valid'][-1], output))\n",
    "        \n",
    "        # Count epochs that model was trained for\n",
    "        model.epochs += 1\n",
    "        \n",
    "        #####################    \n",
    "        # Calculate metrics #\n",
    "        #####################\n",
    "        \n",
    "        # Loss over one epoch is the average of the loss over the entire dataset\n",
    "        metrics_dict['loss']['train'][-1] /= len(train_loader.dataset)\n",
    "        metrics_dict['loss']['valid'][-1] /= len(valid_loader.dataset)\n",
    "\n",
    "        # At the end of the validation, we can calculate metrics to be added to metrics dictionary \n",
    "        for mode in ['train', 'valid']:\n",
    "\n",
    "            # Add accuracy to the metrics dictionary       \n",
    "            pred = np.argmax(metrics_dict['output'][mode][-1], axis=1)       \n",
    "            target = metrics_dict['target'][mode][-1]\n",
    "            correct = (pred == target)\n",
    "            metrics_dict['accuracy'][mode][-1] = sum(correct) / len(correct)\n",
    "\n",
    "            # Add ROC AUC to the metrics dictionary\n",
    "            fpr, tpr, _ = roc_curve(metrics_dict['target'][mode][-1], \n",
    "                                    metrics_dict['output'][mode][-1][:, 1]) # select positive class\n",
    "            metrics_dict['roc_auc'][mode][-1] = auc(fpr, tpr)\n",
    "\n",
    "            # Add PR AUC to the metrics dictionary\n",
    "            pr_auc = average_precision_score(metrics_dict['target'][mode][-1], \n",
    "                                             metrics_dict['output'][mode][-1][:, 1]) # select positive class132 / 024\n",
    "            metrics_dict['pr_auc'][mode][-1] = pr_auc\n",
    "        \n",
    "        # Print emtrics\n",
    "        output_str = [f'\\n\\t\\tTraining loss: {metrics_dict[\"loss\"][\"train\"][-1]:.4f}\\t\\t\\tValidation loss: {metrics_dict[\"loss\"][\"valid\"][-1]:.4f}']\n",
    "        output_str.append(f'\\n\\t\\tTraining accuracy: {metrics_dict[\"accuracy\"][\"train\"][-1]:.4f}\\t\\tValidation accuracy: {metrics_dict[\"accuracy\"][\"valid\"][-1]:.4f}')\n",
    "        output_str.append(f'\\n\\t\\tTraining roc_auc: {metrics_dict[\"roc_auc\"][\"train\"][-1]:.4f}\\t\\tValidation roc_auc: {metrics_dict[\"roc_auc\"][\"valid\"][-1]:.4f}')\n",
    "        output_str.append(f'\\n\\t\\tTraining pr_auc: {metrics_dict[\"pr_auc\"][\"train\"][-1]:.4f}\\t\\t\\tValidation pr_auc: {metrics_dict[\"pr_auc\"][\"valid\"][-1]:.4f}')\n",
    "        \n",
    "        # Save the model if the validation loss decreases\n",
    "        if metrics_dict['loss']['valid'][-1] < valid_loss_min: # valid_loss_min is np.Inf for the first Epoch\n",
    "\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), save_file_name)\n",
    "            output_str.append(f'\\t\\tSaving...')\n",
    "\n",
    "            # Track improvement\n",
    "            epochs_no_improve = 0 # Reset tracker to 0, since the loss decreased\n",
    "            best_epoch = epoch\n",
    "\n",
    "            # Set the validation loss to the current validation loss\n",
    "            valid_loss_min = metrics_dict['loss']['valid'][-1]\n",
    "\n",
    "        # Otherwise increment count of epochs with no improvement\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "            # Trigger early stopping\n",
    "            if epochs_no_improve >= max_epochs_stop:\n",
    "                early_stopping=True\n",
    "                break\n",
    "\n",
    "         # Print every n epochs (e.g. 2, print only even epochs)\n",
    "        if (model.epochs) % print_every == 0:\n",
    "            print(''.join(output_str)+'\\n')        \n",
    "\n",
    "    ########################\n",
    "    # Report final metrics #\n",
    "    ########################\n",
    "    \n",
    "    # Calculate time function took in total\n",
    "    total_time = round(timer() - overall_start)\n",
    "    total_time = str(datetime.timedelta(seconds=total_time))\n",
    "    \n",
    "                \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(save_file_name))  \n",
    "    model.epochs = best_epoch + 1\n",
    "    \n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "\n",
    "    # Report final metrics\n",
    "    early_stopping_str = f'Early Stopping! ' if early_stopping else ''\n",
    "    output_str = [f'{early_stopping_str}Total epochs: {epoch+1} ({total_time}), Best epoch: {model.epochs}, with loss: {metrics_dict[\"loss\"][\"valid\"][model.epochs-1]:.4f}']\n",
    "    output_str.append(f'accuracy: {metrics_dict[\"accuracy\"][\"valid\"][model.epochs-1]:.4f}')\n",
    "    output_str.append(f'roc_auc: {metrics_dict[\"roc_auc\"][\"valid\"][model.epochs-1]:.4f}')\n",
    "    output_str.append(f'pr_auc: {metrics_dict[\"pr_auc\"][\"valid\"][model.epochs-1]:.4f}')\n",
    "    print(', '.join(output_str))\n",
    "\n",
    "    return model, metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do train the model, we have to first define a **loss function**, and an **optimizer**.\n",
    "As a loss function, we use the log loss (a.k.a the CrossEntropyLoss). This is the standard loss used for a binary prediction problem. The optimizer is the type of gradient descent algorithm that we use (Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t100.00% complete (0:00:02)\n",
      "\t\tTraining loss: 0.3461\t\t\tValidation loss: 0.3748\n",
      "\t\tTraining accuracy: 0.8743\t\tValidation accuracy: 0.8668\n",
      "\t\tTraining roc_auc: 0.7726\t\tValidation roc_auc: 0.7134\n",
      "\t\tTraining pr_auc: 0.5084\t\t\tValidation pr_auc: 0.4432\t\tSaving...\n",
      "\n",
      "Epoch: 2\t100.00% complete (0:00:02)\n",
      "\t\tTraining loss: 0.3453\t\t\tValidation loss: 0.3737\n",
      "\t\tTraining accuracy: 0.8743\t\tValidation accuracy: 0.8676\n",
      "\t\tTraining roc_auc: 0.7728\t\tValidation roc_auc: 0.7165\n",
      "\t\tTraining pr_auc: 0.5128\t\t\tValidation pr_auc: 0.4493\t\tSaving...\n",
      "\n",
      "Epoch: 3\t100.00% complete (0:00:02)\n",
      "\t\tTraining loss: 0.3440\t\t\tValidation loss: 0.3763\n",
      "\t\tTraining accuracy: 0.8732\t\tValidation accuracy: 0.8682\n",
      "\t\tTraining roc_auc: 0.7780\t\tValidation roc_auc: 0.7131\n",
      "\t\tTraining pr_auc: 0.5105\t\t\tValidation pr_auc: 0.4441\n",
      "\n",
      "Total epochs: 3 (0:00:06), Best epoch: 2, with loss: 0.3737, accuracy: 0.8676, roc_auc: 0.7165, pr_auc: 0.4493\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# lr is the learning rate, low = slower but more accurate learning, high = faster but less accurate learning\n",
    "# 1e-3 is a good middle ground value\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model, metrics_dict = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['valid'],\n",
    "    save_file_name=f'delete_me.pt',\n",
    "    n_epochs=3,\n",
    "    max_epochs_stop=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use functions to visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_roc(metrics_dict, epoch, mode=\"valid\"):\n",
    "    \n",
    "    assert mode in[\"train\", \"valid\"], 'mode must be \"train\" or \"valid\"'\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    fpr, tpr, _ = roc_curve(metrics_dict['target'][mode][epoch-1],\n",
    "                            metrics_dict['output'][mode][epoch-1][:, 1]) # select positive class\n",
    "    \n",
    "    ax.plot(fpr, tpr, label=f'area = {metrics_dict[\"roc_auc\"][mode][epoch-1]:2.2f}')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC curve')\n",
    "    ax.legend(loc='lower right')\n",
    "    print(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_auc_roc(metrics_dict, epoch=1, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
